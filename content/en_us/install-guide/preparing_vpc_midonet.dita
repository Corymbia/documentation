<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="concept_hgp_4wc_pt">
 <title>Understanding VPC and MidoNet</title>
 <shortdesc>This topic describes MidoNet components and their Eucalyptus deployment options, which
  provide support for VPC on Eucalyptus.</shortdesc>
 <prolog>
  <metadata>
   <keywords>
    <indexterm>networking modes <indexterm>vpcmido</indexterm>
    </indexterm>
   </keywords>
  </metadata>
 </prolog>
 <conbody>
  <p>The Eucalyptus Virtual Private Cloud (VPC) support is implemented with a Software-Defined
   Networking (SDN) technology developed by Midokura, called MidoNet. MidoNet is an open-source
   network virtualization platform for Infrastructure-as-a-Service (IaaS) clouds that implements and
   exposes virtual network components as software abstractions, enabling programmatic provisioning
   of virtual networks. The purpose of this topic is to describe some possible network reference
   architectures for the deployment, configuration, and operation of MidoNet for Eucalyptus
   clouds.</p>
  <section>
   <title>MidoNet Components</title>
   <p>A MidoNet deployment consists of four types of nodes (according to their logical functions or
    services offered), connected via four IP networks as depicted in Figure 1. MidoNet does not
    require any specific hardware, and can be deployed in commodity x86_64 servers. Interactions
    with MidoNet are accomplished through Application Programming Interface (API) calls, which are
    translated into (virtual) network topology changes. Network state information is stored in a
    logically centralized data store, called the Network State Database (NSDB), which is implemented
    on top of two open-source distributed coordination and data store technologies: Zookeeper and
    Cassandra. Implementation of (virtual) network topology is realized via cooperation and
    coordination among MidoNet agents, which are deployed in nodes that participate in MidoNet.</p>
   <image href="images/euca_mido_1.png"
    alt="Figure 1. Logical view of a MidoNet deployment. Four components are connected via four networks."/>
   <p><i>Figure 1: Logical view of a MidoNet deployment. Four components are connected via four
     networks.</i></p>
   <p>Node types:</p>
   <ul>
    <li>MidoNet Network State Database (NSDB): consists of a cluster of Zookeeper and Cassandra. All
     MidoNet nodes must have IP connectivity with NSDB.</li>
    <li>MidoNet API: consists of tomcat and MidoNet web app. Exposes MidoNet REST APIs.</li>
    <li>Hypervisor: MidoNet agent (midolman) are required in all Hypervisors to enable VMs to be
     connected via MidoNet overlay networks/SDN.</li>
    <li>Gateway: Gateway nodes are connected to the public network, and enable the network flow from
     MidoNet overlays to the public network.</li>
   </ul>
   <p>Physical Networks</p>
   <ul>
    <li>NSDB: IP network that connects all nodes that participate in MidoNet. While NSDB and Tunnel
     Zone networks can be the same, it is recommended to have an isolated (physical or VLAN)
     segment.</li>
    <li>API: in Eucalyptus deployments only eucanetd/CLC needs access to the API network. Only
     "special hosts/processes" should have access to this network. The use of "localhost" network on
     the node running CLC/eucanetd is sufficient and recommended in Eucalyptus deployments.</li>
    <li>Tunnel Zone: IP network that transports the MidoNet overlay traffic (Eucalyptus VM traffic),
     which is not "visible" on the physical network.</li>
    <li>Public network: network with access to the Internet (or corporate/enterprise) network.</li>
   </ul>
  </section>
  <section><title>MidoNet Deployment Scale</title>
   <p>Three reference architectures are presented in this document, ordered by complexity and
    size:</p>
   <ul>
    <li>Proof-of-Concept (PoC)</li>
    <li>Production: Small</li>
    <li>Production: Large</li>
   </ul>
   <p>Production: Large reference architecture represents the most complete and recommended
    deployment model of MidoNet for Eucalyptus. Whenever possible (such as when resources are
    available), deployments should closely match with the Production: Large reference architecture
    (even on small scale clouds).</p>
   <p>All MidoNet components are designed and implemented to horizontally scale. Therefore, it is
    possible to start small and add resources as they become available.</p>
  </section>
  <section><title>MidoNet Software Version</title>
   <p>There are currently two distributions of MidoNet:</p>
   <ul>
    <li>Midokura Enterprise MidoNet (commercial version with 24/7 support - 30 day evaluation
     available). Eucalyptus are tested and validated using MEM v1.9 series.</li>
    <li>Open Source MidoNet (available at http://www.midonet.org)</li>
   </ul>
   <p>MEM version 1.9 is currently the recommended/validated version for Eucalyptus deployments.</p>
  </section>
  <section><title>Eucalyptus with MidoNet</title>
   <p>An Eucalyptus with MidoNet deployment consists of the following components:</p>
   <image href="images/euca_mido_2.png"
    alt="Figure 2. Logical view of a Eucalyptus with MidoNet deployment. VM private network is created/virtualized by MidoNet, and 'software-defined' by eucanetd. Ideally, each component and network should have its own set of independent resources. In practice, components are grouped and consolidated into a set of servers, as detailed in different reference architectures."/>
   <p><i>Figure 2: Logical view of a Eucalyptus with MidoNet deployment. VM private network is
     created/virtualized by MidoNet, and 'software-defined' by eucanetd. Ideally, each component and
     network should have its own set of independent resources. In practice, components are grouped
     and consolidated into a set of servers, as detailed in different reference
    architectures.</i></p>
   <p>MidoNet components, Eucalyptus components, and three extra networks are present.</p>
  </section>
  <section><title>Proof of Concept (PoC)</title>
   <p>The PoC reference architecture is designed for very small and transient workloads, typical in
    development and testing environments. Quick deployment with minimal external network
    requirements are the key points of PoC reference architecture. </p>
   <p><b>Requirements</b></p>
   <p>Servers:</p>
   <ul>
    <li>Four (4) or more modern Intel cores or AMD modules - exclude logical cores that share CPU
     resources from the count (Hyperthreads and AMD cores within a module)</li>
    <li>2GB of RAM reserved for MidoNet Agent (when applicable)</li>
    <li>4GB of RAM reserved for MidoNet NSDB (when applicable)</li>
    <li>4GB of RAM reserved for MidoNet API (when applicable)</li>
    <li>30GB of free disk space for NSDB (when applicable)</li>
   </ul>
   <p>Physical Network:</p>
   <ul>
    <li>One (1) 1Gbps IP Network</li>
    <li>A range or list of public IP addresses (Euca_public_IPs)</li>
    <li>Internet Gateway</li>
   </ul>
   <p> Limits: </p>
   <ul>
    <li>Ten (10) MidoNet agents (i.e., 1 Gateway node, 1 CLC, and 8 NCs)</li>
    <li>One (1) MidoNet Gateway</li>
    <li>No fail over, fault tolerance, and/or network load balancing/sharing</li>
   </ul>
   
    <p><b>Deployment Topology</b></p>
   
   <ul>
    <li>Single server with all MidoNet components (NSDB, API, and midolman), and with
     CLC/eucanetd</li>
    <li>A server acting as MidoNet Gateway - when BGP terminated links are used, this node must not
     be co-located with CLC/eucanetd (in a proxy_arp setup described below, it is possible to
     consolidate CLC/eucanetd with MidoNet Gateway). This is due to incompatibilities in
     CentOS/RHEL6 netns (used by eucanetd), and bgpd (started by midolman when BGP links are
     configured).</li>
    <li>Hypervisors with midolman</li>
    <li>One IP network handling NSDB, Tunnel Zone, and Public Network traffic</li>
    <li>API communication via loopback/localhost network</li>
   </ul>
   <image href="images/euca_mido_3.png"
    alt="Figure 3. PoC deployment topology. A single IP network carries NSDB, Tunnel Zone, and Public Network traffic. A single server handles MidoNet NSDB, API (and possibly Gateway) functionality."/>
   <p><i>Figure 3: PoC deployment topology. A single IP network carries NSDB, Tunnel Zone, and
     Public Network traffic. A single server handles MidoNet NSDB, API (and possibly Gateway)
     functionality.</i></p>
   <p><b>MidoNet Gateway Bindings</b></p>
   <p> Three ways to realize MidoNet Gateway bindings are discussed below, starting with the most
    recommended setup. </p>
   <p>Public CIDR block(s) allocated for Eucalyptus (Euca_Public_IPs) needs to be routed to MidoNet
    Gateway by the customer network - this is an environment requirement, outside of control of both
    MidoNet and Eucalyptus systems. One way to accomplish this is to have a BGP terminated link
    available. MidoNet Gateway will establish a BGP session with the customer router to: (1)
    advertise Euca_Public_IPs to the customer router; and (2) get the default route from the
    customer router.</p>
   <p>If a BGP terminated link is not available, but the routing of Euca_Public_IPs is delegated to
    MidoNet Gateway (configuration of customer routing infrastructure), similar setup can be used.
    In such scenario, static routes are configured on the customer router (to route Euca_Public_IPs
    to MidoNet Gateway), and on MidoNet (to use the customer router as the default route).</p>
   <image href="images/euca_mido_4.png"
    alt="Figure 4: How servers are bound to MidoNet in a PoC deployment with BGP. A BGP terminated link is required: the gateway node eth device is bound to MidoNet virtual router (when BGP is involved, the MidoNet Gateway and Euca CLC cannot be co-located). Virtual machine tap devices are bound to MidoNet virtual bridges."/>
   <p><i>Figure 4: How servers are bound to MidoNet in a PoC deployment with BGP. A BGP terminated
     link is required: the gateway node eth device is bound to MidoNet virtual router (when BGP is
     involved, the MidoNet Gateway and Euca CLC cannot be co-located). Virtual machine tap devices
     are bound to MidoNet virtual bridges.</i></p>
   <p>If routed Euca_Public_IPs are not available, static routes on all involved nodes (L2
    connectivity is required among nodes) can be used as illustrated below.</p>
   <image href="images/euca_mido_5.png"
    alt="Figure 5: How servers are bound to MidoNet in a PoC deployment without routed Euca_Public_IPs. Clients that need communication with Euca_Public_IPs configure static routes using MidoNet Gateway as the router. MidoNet Gateway configures a static default route to customer router."/>
   <p><i>Figure 5: How servers are bound to MidoNet in a PoC deployment without routed
     Euca_Public_IPs. Clients that need communication with Euca_Public_IPs configure static routes
     using MidoNet Gateway as the router. MidoNet Gateway configures a static default route to
     customer router.</i></p>
   <p>In the case nodes outside the public network broadcast domain (L2) needs to access
    Euca_Public_IPs, a setup using proxy_arp, as illustrated below, can be used.</p>
   <image href="images/euca_mido_6.png"
    alt="Figure 6: How servers are bound to MidoNet in a PoC deployment with proxy_arp. When routed Euca_Public_IPs are not available, the gateway node should proxy arp for public IP addresses allocated for Eucalyptus, and forward to a veth device that is bound to a MidoNet virtual router. Virtual machine tap devices are bound to MidoNet virtual bridges."/>
   <p><i>Figure 6: How servers are bound to MidoNet in a PoC deployment with proxy_arp. When routed
     Euca_Public_IPs are not available, the gateway node should proxy arp for public IP addresses
     allocated for Eucalyptus, and forward to a veth device that is bound to a MidoNet virtual
     router. Virtual machine tap devices are bound to MidoNet virtual bridges.</i></p>
  </section>
  <section><title>Production: Small</title>
   <p>The Production: Small reference architecture is designed for small scale production quality
    deployments. It supports MidoNet NSDB fault tolerance (partial failures), and limited MidoNet
    Gateways fail-over and load balancing/sharing.</p>
   <p>Boarder Gateway Protocol (BGP) terminated uplinks are recommended for production quality
    deployments.</p>
   <p><b>Requirements</b></p>
   <p>Servers:</p>
   <ul>
    <li>Four (4) or more modern Intel cores or AMD modules - exclude logical cores that share CPU
     resources from the count (Hyperthreads and AMD cores within a module) - for gateway nodes, 4 or
     more cores should be dedicated to MidoNet agent (midolman)</li>
    <li>4GB of RAM reserved for MidoNet Agent (when applicable), 8GB for Gateway nodes</li>
    <li>4GB of free RAM reserved for MidoNet NSDB (when applicable)</li>
    <li>4GB of free RAM reserved for MidoNet API (when applicable)</li>
    <li>30GB of free disk space for NSDB (when applicable)</li>
    <li>Two (2) 10Gbps NICs per server</li>
    <li>Three (3) servers dedicated to MidoNet NSDB</li>
    <li>Two (2) servers as MidoNet Gateways</li>
   </ul>
   <p>Physical Network:</p>
   <ul>
    <li>One (1) 10Gbps IP Network for public network (if upstream links are 1Gbps, this could be
     1Gbps)</li>
    <li>One (1) 10Gbps IP Network for Tunnel Zone and NSDB</li>
    <li>Public Classless Inter-Domain Routing (CIDR) block (Euca_public_IPs)</li>
    <li>Two (2) BGP terminated uplinks</li>
   </ul>
   <p>Limits:</p>
   <ul>
    <li>Thirty two (32) MidoNet agents (i.e., 2 Gateway nodes and 30 Hypervisors)</li>
    <li>Two (2) MidoNet Gateways</li>
    <li>Tolerate 1 NSDB server failure</li>
    <li>Tolerate 1 MidoNet Gateway/uplink failure</li>
    <li>Limited uplinks load sharing/balancing</li>
   </ul>
   <p><b>Deployment Topology</b></p>
   <ul>
    <li>A 3-node cluster for NSDB (co-located Zookeeper and Cassandra)</li>
    <li>eucanetd co-located with MidoNet API Server (Tomcat)</li>
    <li>Two (2) MidoNet Gateway Nodes</li>
    <li>Hypervisors with midolman</li>
    <li>One 10Gbps IP network handling NSDB and Tunnel Zone traffic</li>
    <li>One 10Gbps IP Network handling Public Network traffic</li>
    <li>API communication via loopback/localhost network</li>
   </ul>
   <image href="images/euca_mido_7.png"
    alt="Figure 7: Production:Small deployment topology. A 10Gbps IP network carries NSDB and Tunnel Zone traffic. Another 10Gbps IP network carries Public Network traffic. A 3-node cluster for NSDB tolerates 1 server failure, and 2 gateways enable network fail-over and limited load balancing/sharing."/>
   <p><i>Figure 7: Production:Small deployment topology. A 10Gbps IP network carries NSDB and Tunnel
     Zone traffic. Another 10Gbps IP network carries Public Network traffic. A 3-node cluster for
     NSDB tolerates 1 server failure, and 2 gateways enable network fail-over and limited load
     balancing/sharing.</i></p>
   <image href="images/euca_mido_8.png"
    alt="Figure 8: How servers are bound to MidoNet in a Production:Small deployment. Gateway Nodes have physical devices bound to a MidoNet virtual router. These devices should have L2 and L3 connectivity to the Customer's Router, and with BGP terminated links. Virtual machine tap devices are bound to MidoNet virtual bridges."/>
   <p><i>Figure 8: How servers are bound to MidoNet in a Production:Small deployment. Gateway Nodes
     have physical devices bound to a MidoNet virtual router. These devices should have L2 and L3
     connectivity to the Customer's Router, and with BGP terminated links. Virtual machine tap
     devices are bound to MidoNet virtual bridges.</i></p>
   <p><b>NSDB Data Replication</b></p>
   <ul>
    <li>NSDB is deployed in a cluster of 3 nodes</li>
    <li>Zookeeper and Cassandra both have built-in data replication</li>
    <li>One server failure is tolerated</li>
   </ul>
   <p><b>MidoNet Gateway Failover</b></p>
   <ul>
    <li>Two paths are available to and from MidoNet, and failover is handled by BGP</li>
   </ul>
   <p><b>MidoNet Gateway Load Balancing and Sharing</b></p>
   <ul>
    <li>Load Balancing from MidoNet is implemented by MidoNet agents (midolman): ports in a stateful
     port group with default routes out are used in a round-robin fashion.</li>
    <li>Partial load sharing from the Customer's router to MidoNet can be accomplished by: <ul>
      <li>Partition the allocated CIDR in 2 parts. For example, a /24 CIDR can be split into 2 /25
       CIDRs.</li>
      <li>One MidoNet BGP port should advertise the top half (/25) and /24; the other advertises the
       bottom half (/25) and /24.</li>
      <li>When both ports are operational, routing will favor the most specific route (i.e., /25).
       If a port fails, the /24 will be used instead.</li>
     </ul></li>
   </ul>
  </section>
  <section><title>Production: Large</title>
   <p>The Production:Large reference architecture is designed for large scale (500 to 600 MidoNet
    agents) production quality deployments. It supports MidoNet NSDB fault tolerance (partial
    failures), and MidoNet Gateways fail-over and load balancing/sharing. </p>
   <p>Boarder Gateway Protocol (BGP) terminated uplinks are required. Each uplink should come from
    an independent router. </p>
   <p><b>Requirements:</b></p>
   <ul>
    <li>Eight (8) or more modern Intel cores or AMD modules - exclude logical cores that share CPU
     resources from the count (Hyperthreads and AMD cores within a module) - for gateway nodes, 8 or
     more cores should be dedicated to MidoNet agent (midolman)</li>
    <li>4GB of RAM reserved for MidoNet Agent (when applicable), 16GB for Gateway nodes</li>
    <li>4GB of free RAM reserved for MidoNet NSDB (when applicable)</li>
    <li>4GB of free RAM reserved for MidoNet API (when applicable)</li>
    <li>30GB of free disk space for NSDB (when applicable)</li>
    <li>One 1Gbps and 2 10Gbps NICs per server</li>
    <li>Five (5) servers dedicated to MidoNet NSDB</li>
    <li>Three (3) servers as MidoNet Gateways</li>
   </ul>
   <p>Physical Network:</p>
   <ul>
    <li>One 1Gbps IP Network for NSDB</li>
    <li>One 10Gbps IP Network for public network (if upstream links are 1Gbps, this could be
     1Gbps)</li>
    <li>One 10Gbps IP Network for Tunnel Zone</li>
    <li>Public Classless Inter-Domain Routing (CIDR) block (Euca_public_IPs)</li>
    <li>Three (3) BGP terminated uplinks, each of which coming from an independent router</li>
   </ul>
   <p>Limits:</p>
   <ul>
    <li>500 to 600 MidoNet agents</li>
    <li>Three (3) MidoNet Gateways</li>
    <li>Tolerate 1 to 2 NSDB server failures</li>
    <li>Tolerate 1 to 2 MidoNet Gateway/uplink failures</li>
   </ul>
   <p><b>Deployment Topology</b></p>
   <ul>
    <li>A 5-node cluster for NSDB (co-located Zookeeper and Cassandra)</li>
    <li>eucanetd co-located with MidoNet API Server (Tomcat)</li>
    <li>Three (3) MidoNet Gateway Nodes</li>
    <li>Hypervisors with midolman</li>
    <li>One 1Gbps IP network handling NSDB traffic</li>
    <li>One 10Gbps IP network handling Tunnel Zone traffic</li>
    <li>One 10Gbps IP network handling Public Network traffic</li>
    <li>API communication via loopback/localhost network</li>
   </ul>
   <image href="images/euca_mido_9.png"
    alt="Figure 9: Production:Large deployment topology. A 1Gbps IP network carries NSDB; a 10Gbps IP network carries Tunnel Zone traffic; and another 10Gbps IP network carries Public Network traffic. A 5-node cluster for NSDB tolerates 2 server failures, and 3 gateways enable network fail-over and load balancing/sharing.
    Servers are bound to MidoNet in a way similar to Production:Small."/>
   <p><i>Figure 9: Production:Large deployment topology. A 1Gbps IP network carries NSDB; a 10Gbps
     IP network carries Tunnel Zone traffic; and another 10Gbps IP network carries Public Network
     traffic. A 5-node cluster for NSDB tolerates 2 server failures, and 3 gateways enable network
     fail-over and load balancing/sharing. Servers are bound to MidoNet in a way similar to
     Production:Small. </i></p>
   <p><b>NSDB Data Replication</b></p>
   <ul>
    <li>NSDB is deployed in a cluster of 5 nodes</li>
    <li>Zookeeper and Cassandra both have built-in data replication</li>
    <li>Up to 2 server failures tolerated</li>
   </ul>
   <p><b>MidoNet Gateway Failover</b></p>
   <ul>
    <li>Three paths are available to and from MidoNet, and failover is handled by BGP</li>
   </ul>
   <p><b>MidoNet Gateway Load Balancing/Sharing</b></p>
   <ul>
    <li>Load Balancing from MidoNet is implemented by MidoNet agents (midolman): ports in a stateful
     port group with default routes out are used in a round-robin fashion.</li>
    <li>The customer AS should handle multi path routing in order to support load sharing/balancing
     to MidoNet; for example, Equal Cost Multi Path (ECMP).</li>
   </ul>
  </section>
 </conbody>
</concept>
