<?xml version="1.0" encoding="UTF-8"?>
<!--This work by Eucalyptus Systems is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. See the accompanying LICENSE file for more information.-->
<!DOCTYPE task PUBLIC "-//OASIS//DTD DITA Task//EN" "task.dtd">
<task id="configure_storage_controller" props="subscribe">
	<title>Configure the Storage Controller</title>
	<shortdesc>Eucalyptus offers SAN support for Eucalyptus block storage (EBS). Eucalyptus directs
		the Storage Controller (SC) to manage any supported SAN devices or third-party clients.
		Eucalyptus automatically creates and tears down volumes, snapshots, and data connections
		from guest instances. The administrator does not need to pre-allocate volumes or LUNs for
		Eucalyptus.</shortdesc>
	<taskbody>
		<context>
			<p/>
			<p>Eucalyptus currently offers several backend providers for the Storage Controller:</p>
			<ul>
				<li>Overlay</li>
				<li>DAS</li>
				<li>Equallogic</li>
				<li>Netapp</li>
				<li>EMC-VNX</li>
				<li>Ceph-RBD</li>
			</ul>
			<p>You must configure the SC to use one of these options. The SC automatically goes to
				the <codeph>BROKEN</codeph> state after being registered with the CLC and will
				remain in that state until you explicitly configure the SC by telling it which
				backend storage provider to use.</p>
			<p>You can check the state of a storage controller by running
					<codeph>euca-describe-services -E</codeph> and note the state and status message
				of the SC(s). The output for an unconfigured SC will look like:</p>
			<p>
				<codeblock>SERVICE	storage        	PARTI00        	SC71           	BROKEN    	37  	http://192.168.51.71:8773/services/Storage	arn:euca:eucalyptus:PARTI00:storage:SC71/
SERVICEEVENT	6c1f7a0a-21c9-496c-bb79-23ddd5749222	arn:euca:eucalyptus:PARTI00:storage:SC71/
SERVICEEVENT	6c1f7a0a-21c9-496c-bb79-23ddd5749222	ERROR
SERVICEEVENT	6c1f7a0a-21c9-496c-bb79-23ddd5749222	Sun Nov 18 22:11:13 PST 2012
SERVICEEVENT	6c1f7a0a-21c9-496c-bb79-23ddd5749222	SC blockstorageamanger not configured. Found empty or unset manager(unset). Legal values are: das,overlay</codeblock></p>
			<p>Note the error above: <codeph>SC blockstoragemanager not configured. Found empty or
					unset manager(unset). Legal values are: das,overlay</codeph>.</p>
			<p>This indicates that the SC is not yet configured. It can be configured by setting the
					<codeph>[partition].storage.blockstoragemanager</codeph> property to either
				'das' or 'overlay'.</p>
			<p>If you have installed the Eucalyptus Enterprise packages for your EBS adapter, you
				will also see additional options in the output line above, and can set the block
				storage manager to 'netapp','emc-vnx-flare31','emc-vnx', 'equallogic', or 'ceph-rbd'
				as appropriate.</p>
			<p>You can verify that the SC blockstoragemanager is unset using:
				<codeblock>euca-describe-properties | grep blockstorage</codeblock>
			</p>
			<p>To configure SAN support, follow the steps for your desired backend storage device:
					<xref href="#enabling_overlay">Open-Source ISCSI Filesystem-backed</xref>, <xref
					href="#enabling_san_dell">Dell Equallogic</xref>, <xref
					href="#enabling_san_jbod">JBOD</xref>, <xref href="#enabling_san_netapp"
					>Netapp</xref>, <xref href="#enabling_san_emc_vnx">EMC VNX</xref>, or <xref
					href="#enabling_san_ceph"/>.</p>
		</context>
	</taskbody>
	<task id="enabling_overlay">
		<title>Configuring the SC to use the local filesystem (Overlay)</title>
		<shortdesc>This was the default configuration option for the SC in pre-3.2 Eucalyptus. In
			this configuration the SC itself hosts the volume and snapshots for EBS and stores them
			as files on the local filesystem. It uses standard linux iSCSI tools to serve the
			volumes to instances running on NCs.</shortdesc>
		<taskbody>
			<steps>
				<step>
					<cmd>Configure the SC to use the OverlayManager for storage.</cmd>
					<info>
						<codeblock>euca-modify-property -p &lt;partition>.storage.blockstoragemanager=overlay</codeblock>
						The output of the command should be similar to:
						<codeblock>PROPERTY	PARTI00.storage.blockstoragemanager	overlay was &lt;unset></codeblock>
					</info>
				</step>
				<step>
					<cmd>Verify that the property value is now: 'overlay'</cmd>
					<info>
						<codeblock>euca-describe-properties | grep blockstorage</codeblock>
					</info>
				</step>
			</steps>
		</taskbody>
	</task>
	<task id="enabling_san_dell">
		<title>Enable Dell Equallogic SANs</title>
		<shortdesc/>
		<taskbody>
			<steps>
				<step>
					<cmd>Configure the SC to use the EquallogicManager for storage.</cmd>
					<info>
						<codeblock>euca-modify-property -p &lt;partition>.storage.blockstoragemanager=equallogic</codeblock>
						The output of the command should be similar to:
						<codeblock>PROPERTY	PARTI00.storage.blockstoragemanager	equallogic was &lt;unset></codeblock>
					</info>
				</step>
				<step>
					<cmd>Verify that the propery value is now: 'equallogic'</cmd>
					<info>
						<codeblock>euca-describe-properties | grep blockstorage</codeblock>
					</info>
				</step>
				<step conref="../shared/shared_task.dita#shared_task/verify_sc">
					<cmd/>
				</step>
				<step>
					<cmd>On the CLC, enable SAN support in Eucalyptus by entering your SAN's
						hostname or IP address, the username, password, and the name of the chap
						user:</cmd>
					<info>
						<codeblock>euca-modify-property -p [partition_name].storage.sanhost=[SAN_IP_address]
euca-modify-property -p [partition_name].storage.sanuser=[SAN_admin_user_name]
euca-modify-property -p [partition_name].storage.sanpassword=[SAN_admin_password]
euca-modify-property -p &lt;partition>.storage.chapuser=[chap_username]</codeblock>
						<p>If you have multiple management IP addresses for the SAN adapter, provide
							a comma-delimited list of IP addresses to the
								<codeph>[partition_name].storage.sanhost</codeph> property.</p>
					</info>
				</step>
			</steps>
			<result>Your Equallogic SAN is now ready to use with Eucalyptus.</result>
		</taskbody>
	</task>

	<task id="enabling_san_jbod" product="jbod">
		<title>Enable Direct Attached Storage (JBOD) SANs</title>
		<shortdesc/>
		<taskbody>
			<context>
				<note type="important">Direct Attached Storage still requires that
						<filepath>/var/lib/eucalyptus/volumes</filepath> has enough space for
					locally cached snapshots.</note>
			</context>
			<steps>
				<step>
					<cmd>Configure the SC to use the (Direct Attached Storage) DASManager for
						storage.</cmd>
					<info>
						<codeblock>euca-modify-property -p &lt;partition>.storage.blockstoragemanager=das</codeblock>
						The output of the command should be similar to:
						<codeblock>PROPERTY	PARTI00.storage.blockstoragemanager	das was &lt;unset></codeblock>
					</info>
				</step>
				<step>
					<cmd>Verify that the propery value is now: 'das'</cmd>
					<info>
						<codeblock>euca-describe-properties | grep blockstorage</codeblock>
					</info>
				</step>
				<step conref="../shared/shared_task.dita#shared_task/verify_sc">
					<cmd/>
				</step>
				<step>
					<cmd>On the CLC, set the DAS device name property. The device name can be either
						a raw device (/dev/sdX, for example), or the name of an existing Linux LVM
						volume group.</cmd>
					<info>
						<codeblock>euca-modify-property -p &lt;cluster name&gt;.storage.dasdevice=&lt;device name&gt;</codeblock>
					</info>
					<stepxmp>
						<p>For example:</p>
						<codeblock>euca-modify-property -p cluster0.storage.dasdevice=/dev/sdb</codeblock>
					</stepxmp>
				</step>
			</steps>
			<result>Your SAN is now ready to use with Eucalyptus.</result>
		</taskbody>
	</task>


	<task id="enabling_san_netapp_overview">
		<title>Enable NetApp SANs</title>
		<shortdesc>Eucalyptus supports both NetApp Clustered ONTAP and traditional 7-mode SANs.
			NetApp Vservers and 7-mode Filers are managed by Eucalyptus using NetApp Manageability
			Software Development Kit (NMSDK) and Data ONTAP APIs. This section covers enabling both
			NetApp Clustered ONTAP and traditional 7-mode SANs.</shortdesc>
		<task id="enabling_san_netapp">
			<title>Enable NetApp 7-mode SANs</title>
			<shortdesc/>
			<taskbody>
				<context>
					<p/>
					<p>To configure NetApp 7-mode Filer and enable the SAN in Eucalyptus:</p>
				</context>
				<steps>
					<step>
						<cmd>Verify Data ONTAP version for the 7-mode Filer is 7.3.3 or later.</cmd>
					</step>
					<step>
						<cmd>Verify SSL access by typing <codeph>secureadmin status</codeph></cmd>
					</step>
					<step>
						<cmd>If SSL is marked inactive, enable with <codeph>secureadmin setup
								ssl</codeph> and generate a new certificate.</cmd>
					</step>
					<step>
						<cmd>Turn on SSL access with <codeph>options httpd.admin.ssl.enable
								on</codeph></cmd>
					</step>
					<step>
						<cmd>Enable the iSCSI service on the NetApp device with <codeph>option
								iscsi.enable on</codeph> or <codeph>option
								licensed_feature.iscsi.enable on</codeph> if you have an embedded
							license on your array.</cmd>
					</step>
					<step>
						<cmd>Turn on the iSCSI service with <codeph>iscsi start</codeph></cmd>
					</step>
					<step>
						<cmd>Enable the iSCSI service on the NetApp device with <codeph>enable iscsi
								service</codeph></cmd>
					</step>
					<step>
						<cmd>Verify that an aggregate with sufficient spare capacity exists.</cmd>
						<choices>
							<choice>If you have SSH access to the NetApp Filer, enter <codeph>aggr
									show_space</codeph>.</choice>
							<choice>If an aggregate with spare capacity does not exist, create one
								using the <codeph>aggr create</codeph> command.</choice>
						</choices>
					</step>
					<step>
						<cmd>Verify that you have a license for FlexClone installed. At the shell
							prompt, enter <codeph>license</codeph> to see the list of all installed
							licenses.</cmd>
					</step>
					<step>
						<cmd>Verify that administrator account credentials for NetApp Filer are
							available to be configured in Eucalyptus. If not, create a new
							administrator account for use by Eucalyptus </cmd>
					</step>
					<step>
						<cmd>Configure the SC to use the NetappManager for storage.</cmd>
						<info>
							<codeblock>euca-modify-property -p &lt;partition>.storage.blockstoragemanager=netapp</codeblock>
							The output of the command should be similar to:
							<codeblock>PROPERTY	&lt;partition>.storage.blockstoragemanager	netapp was &lt;unset></codeblock>
						</info>
					</step>
					<step>
						<cmd>Verify that the propery value is now: 'netapp'</cmd>
						<info>
							<codeblock>euca-describe-properties | grep blockstorage</codeblock>
						</info>
					</step>
					<step conref="../shared/shared_task.dita#shared_task/verify_sc">
						<cmd/>
					</step>
					<step>
						<cmd>Wait for the SC to transition to the NOTREADY or DISABLED state.</cmd>
					</step>
					<step>
						<cmd>On the CLC, enable NetApp SAN support in Eucalyptus by entering the
							Filer's hostname or IP address, the username and password of the
							administrator account, and CHAP username. </cmd>
						<info>
							<note>Eucalyptus uses Challenge Handshake Authentication Protocol (CHAP)
								for disk operations. The CHAP username can be any value, however it
								should be unique when sharing a NetApp Filer across multiple
								Eucalyptus clusters. </note>
							<note>CHAP support for NetApp has been added in Eucalyptus 3.3. An SC
								will not transition to ENABLED state until the CHAP username is
								conﬁgured. </note>
							<codeblock>euca-modify-property -p &lt;partition>.storage.sanhost=&lt;Filer_IP_address>
euca-modify-property -p &lt;partition>.storage.sanuser=&lt;Filer_admin_username>
euca-modify-property -p &lt;partition>.storage.sanpassword=&lt;Filer_admin_password>
euca-modify-property -p &lt;partition>.storage.chapuser=&lt;Chap_username></codeblock>
						</info>
					</step>
					<step>
						<cmd>Wait for the SC to transition to the ENABLED state.</cmd>
						<info>
							<note>The SC must be in the ENABLED state before configuring the
								following properties.</note>
						</info>
					</step>
					<step>
						<cmd>If no aggregate is set, Eucalyptus will query the NetApp Filer for all
							available aggregates and use the one that has the highest capacity (free
							space) by default. To make Eucalyptus use specific aggregate(s)
							configure the following property: </cmd>
						<info>
							<codeblock>euca-modify-property -p &lt;partition>.storage.aggregate=&lt;aggregate_1_name,aggregate_2_name,...></codeblock>
						</info>
						<info>
							<p>If you want Eucalyptus to use the smallest aggregate first configure
								the following property: </p>
							<codeblock>euca-modify-property -p &lt;partition>.storage.uselargestaggregate=false</codeblock>
						</info>
					</step>
					<step>
						<cmd>Set the iSCSI data IP on the ENABLED CLC. This IP is used by NCs to
							perform disk operations on the Filer. </cmd>
						<info>
							<note>Filer IP address can be used as the data port IP. If this is not
								set, Eucalyptus will automatically use the Filer IP
								address/hostname.</note>
							<note>Eucalyptus does not support Multipath I/O for NetApp 7-mode
								Filers. </note>
							<codeblock>euca-modify-property -p &lt;partition>.storage.ncpaths=&lt;ip></codeblock>
						</info>
					</step>
					<step>
						<cmd>Set the iSCSI data IP on the ENABLED CLC. This IP is used by the SC to
							perform disk operations on the Filer. The SC connects to the Filer in
							order to transfer snapshots to Walrus during snapshot operations. </cmd>
						<info>
							<note>The Filer IP address can be used as the data port IP. If this is
								not set, Eucalyptus will automatically use the Filer IP
								address/hostname.</note>
							<note>Eucalyptus does not support Multipath I/O for NetApp 7-mode
								Filers. </note>
							<codeblock>euca-modify-property -p &lt;partition>.storage.scpaths=&lt;ip></codeblock>
						</info>
					</step>
				</steps>
				<result>Your Netapp 7-mode SAN is now ready to use with Eucalyptus.</result>
			</taskbody>
		</task>

		<task id="enabling_san_netapp_ontap">
			<title>Enable NetApp Clustered Data ONTAP SAN</title>
			<shortdesc/>
			<taskbody>
				<context>
					<p>Eucalyptus integrates with NetApp Clustered ONTAP SAN by operating against a
						Vserver. SC must be conﬁgured to operate against Vserver contained in the
						NetApp Clustered ONTAP environment. </p>
					<p>For more information on NetApp Clustered Data ONTAP, see <xref
							href="http://www.netapp.com/us/system/pdf-reader.aspx?m=tr-3982.pdf&amp;cc=us"
							scope="external" format="pdf">Clustered Data ONTAP 8.1 and 8.1.1: An
							Introduction</xref>.</p>
					<p>To configure NetApp Vserver and enable the SAN in Eucalyptus:</p>
				</context>
				<steps>
					<step>
						<cmd>Verify Clustered Data ONTAP version for the SAN is 8.1.1 or
							later.</cmd>
					</step>
					<step>
						<cmd> Verify that FlexClone and iSCSI licenses are installed on the SAN.
						</cmd>
					</step>
					<step>
						<cmd> Verify that a Vserver with iSCSI data protocol is available for use by
							Eucalyptus. </cmd>
					</step>
					<step>
						<cmd>Verify that Vserver administration is delegated to a user with
							administrative privileges for that Vserver. If not, create a new new
							Vserver administrator account for use by Eucalyptus. </cmd>
					</step>
					<step>
						<cmd> Verify that a management (only) Logical Interface (LIF) is configured
							for the Vserver and an IP address or hostname is assigned to it. </cmd>
					</step>
					<step>
						<cmd> Verify that data LIFs are configured on the Vserver. </cmd>
					</step>
					<step>
						<cmd> Verify that one or more aggregates with sufficient spare capacity
							exists. </cmd>
					</step>
					<step>
						<cmd>Verify the network connectivity between Eucalyptus components and the
							Vserver. The SC must be able communicate with the Vserver over both
							management and data LIFs. The NC must be able to communicate with the
							Vserver using the data LIFs. </cmd>
					</step>
					<step>
						<cmd>Configure the SC to use the NetApp SAN for storage: </cmd>
						<info>
							<codeblock>euca-modify-property -p &lt;partition>.storage.blockstoragemanager=netapp</codeblock>
							<p>The output of the command should be similar to:</p>
							<codeblock>PROPERTY &lt;partition>.storage.blockstoragemanager netapp was &lt;unset></codeblock>
						</info>
					</step>
					<step>
						<cmd>Verify that the propery value is now: 'netapp' </cmd>
						<info><codeblock>euca-describe-properties | grep blockstorage </codeblock></info>
					</step>
					<step>
						<cmd>On the CLC, run the following command to verify that the SC is listed;
							note that it may be in the BROKEN state: </cmd>
						<info>
							<codeblock>euca_conf --list-scs</codeblock>
						</info>
					</step>
					<step>
						<cmd>Wait for the SC to transition to NOTREADY or DISABLED states. </cmd>
					</step>
					<step>
						<cmd>On the CLC, enable NetApp SAN support in Eucalyptus by entering the
							Vserver's hostname or IP address, the username and password of the
							administrator account, CHAP username and Vserver name. </cmd>
						<info>
							<note>Eucalyptus uses Challenge Handshake Authentication Protocol (CHAP)
								for disk operations. The CHAP username can be any value, however it
								should be unique when sharing a NetApp Vserver across multiple
								Eucalyptus clusters.</note>
							<note>CHAP support for NetApp has been added in Eucalyptus 3.3. The SC
								will not transition to ENABLED state until the CHAP username is
								conﬁgured.</note>
							<codeblock>euca-modify-property -p &lt;partition>.storage.sanhost=&lt;Vserver_IP_address>
euca-modify-property -p &lt;partition>.storage.sanuser=&lt;Vserver_admin_username>
euca-modify-property -p &lt;partition>.storage.sanpassword=&lt;Vserver_admin_password>
euca-modify-property -p &lt;partition>.storage.chapuser=&lt;Chap_username></codeblock>
							<note> The following command may fail if tried immediately after
								conﬁguring the block storage manager. Retry the command a few times,
								pausing for a few seconds after each retry: </note>
							<codeblock>euca-modify-property -p &lt;partition>.storage.vservername=&lt;Vserver_name></codeblock>
						</info>
					</step>
					<step>
						<cmd> Wait for the SC to transition to ENABLED state. </cmd>
						<info>
							<note>The SC must be in the ENABLED state before configuring the
								following properties.</note>
						</info>
					</step>
					<step>
						<cmd> If no aggregate is set, Eucalyptus will query the NetApp Vserver for
							all available aggregates and use the one that has the highest capacity
							(free space) by default. To make Eucalyptus use specific aggregate(s)
							configure the following property: </cmd>
						<info>
							<codeblock>euca-modify-property -p &lt;partition>.storage.aggregate=&lt;aggregate_1_name,
								aggregate_2_name,...></codeblock>
						</info>
						<info>
							<p>If you want Eucalyptus to use the smallest aggregate first configure
								the following property: </p>
							<codeblock>euca-modify-property -p &lt;partition>.storage.uselargestaggregate=false</codeblock>
						</info>
					</step>
					<step>
						<cmd>Set an IP address for the iSCSI data LIF on the ENABLED CLC. This is
							used for NCs performing disk operations on the Vserver. If you want to
							configure multiple IPs, see see <xref
								href="configure_netapp_multipath.dita">Configure NetApp
								Multipathing</xref>.</cmd>
						<info>
							<codeblock>euca-modify-property -p &lt;partition>.storage.ncpaths=&lt;ip></codeblock>
						</info>
					</step>
					<step>
						<cmd>Set an IP address for the iSCSI data LIF on the ENABLED CLC. This is
							used by the SC for performing disk operations on the Vserver. The SC
							connects to the data LIFs on the Vserver in order to transfer snapshots
							to Walrus during snapshot operations. If you want to configure multiple
							IPs, see <xref href="configure_netapp_multipath.dita">Configure NetApp
								Multipathing</xref>.</cmd>
						<info>
							<codeblock>euca-modify-property -p &lt;partition>.storage.scpaths=&lt;ip></codeblock>
						</info>
					</step>
				</steps>
				<result>Your NetApp Clustered Data ONTAP SAN is now ready to use with
					Eucalyptus.</result>
			</taskbody>
		</task>
	</task>


	<task id="enabling_san_emc_vnx">
		<title>Enable EMC VNX SANs</title>
		<taskbody>
			<context>
				<p>This adapter uses the newer VNX-Snapshot feature available on VNX devices running
					FLARE v5.32 or later that have a VNX-Snapshot license. This adapter also
					requires the Navisphere Secure CLI to be installed on the SCs. The Navisphere
					CLI must be version 7.32.0.5.54 or later.</p>
				<p>
					<note type="important">You must create a Clone Private LUN (CPL) of at least 1GB
						on each SP. For more information on creating private LUNs, see the VNX
						documentation.</note>
				</p>
			</context>
			<steps>

				<step>
					<cmd>We assume that the Navisphere CLI is installed in
							<filepath>/opt/Navisphere</filepath> on the SC.</cmd>
					<info>
						<note type="important">Eucalyptus currently supports version 7.32.0.5.54 or
							later of the Navisphere CLI.</note>
					</info>
				</step>
				<step>
					<cmd>Verify that the CLI is installed and can communicate with the VNX from the
						SCs.</cmd>
					<info>On each SC that you are configuring, test the naviseccli command as
						follows:
						<codeblock>/opt/Navisphere/bin/naviseccli -User &lt;your SAN username> -Password &lt;your SAN password> -Scope 0 -Address &lt;management port IP> connection -pingnode -address &lt;a data port IP on your VNX></codeblock>
						Verify that the command runs successfully and the ping gets replies from the
						SAN. </info>
				</step>
				<step conref="../shared/shared_task.dita#shared_task/verify_sc">
					<cmd/>
				</step>
				<step>
					<cmd>Configure the SC to use the EMC VNX VNX-Snapshot-based manager for
						storage.</cmd>
					<info>
						<codeblock>euca-modify-property -p &lt;partition>.storage.blockstoragemanager=emc-vnx</codeblock>
						The output of the command should be similar to:
						<codeblock>PROPERTY	PARTI00.storage.blockstoragemanager	emc-vnx was &lt;unset></codeblock>
					</info>
				</step>
				<step>
					<cmd>Check the SC to be sure that it has transitioned out of the BROKEN state
						and is in either NOTREADY or DISABLED before configuring the rest of the
						properties for the SC. The following commands should be run on the ENABLED
						CLC to configure the SC.</cmd>
					<info>On the ENABLED CLC, run:
						<codeblock>euca_conf --list-scs</codeblock></info>
				</step>
				<step conref="../shared/shared_task.dita#shared_task/enable_san">
					<cmd/>
				</step>
				<step>
					<cmd>On the ENABLED CLC, set the login scope for the command line access. For
						most installs, the login scope will be <codeph>0</codeph>, which indicates a
						global login scope for the device. <codeph>1</codeph> indicates a local
						scope. <codeph>2</codeph> indicates LDAP authentication for the SAN device.
						Use login scope value of <codeph>2</codeph> only if your SAN is configured
						to use LDAP authentication and you have an admin user configured to use
						LDAP.</cmd>
					<info>
						<codeblock>euca-modify-property -p &lt;partition_name>.storage.loginscope=&lt;login_scope></codeblock>
					</info>
				</step>
				<step>
					<cmd>On the ENABLED CLC, set the username for the Challenge Handshake
						Authentication Protocol (CHAP). This can be any value, however it should be
						unique when sharing VNX on multiple Eucalyptus clusters.</cmd>
					<info>
						<codeblock>euca-modify-property -p &lt;partition_name>.storage.chapuser=&lt;chap_username></codeblock>
					</info>
				</step>
				<step>
					<cmd>On the ENABLED CLC, set the value for the unique storage pool that you have
						configured to use with the SC.</cmd>
					<info>
						<codeblock>euca-modify-property -p &lt;partition_name>.storage.storagepool=0</codeblock>
					</info>
				</step>
				<step>
					<cmd>On the ENABLED CLC, set the iSCSI data port IP for NCs to use to perform
						disk operations on the SAN. If you want to configure multiple IPs, see <xref
							href="configure_emc_vnx_multipath.dita">Configure EMC VNX
							Multipathing</xref>.</cmd>
					<info>
						<codeblock>euca-modify-property -p &lt;partition_name>.storage.ncpaths=&lt;ip></codeblock>
					</info>
				</step>
				<step>
					<cmd>On the ENABLED CLC, set the iSCSI data port IP for SCs to use to perform
						disk operations on the SAN. The SCs connect to the data ports on the SAN in
						order to transfer snapshots to Walrus during snapshot operations. If you
						want to configure multiple IPs, see the section on 'multipathing.</cmd>
					<info>
						<codeblock>euca-modify-property -p &lt;partition_name>.storage.scpaths=&lt;ip></codeblock>
					</info>
				</step>
				<step>
					<cmd>On the ENABLED CLC, set the path to Navisphere CLI that you downloaded
						earlier to the SC. The following example shows the default path. This is
						that path on the SC, not on the CLC.</cmd>
					<info>
						<codeblock>euca-modify-property -p &lt;partition_name>.storage.clipath=/opt/Navisphere/bin/naviseccli</codeblock>
					</info>
				</step>
			</steps>
			<result>Your EMC VNX SAN is now ready to use with Eucalyptus. <note type="tip">Note: The
					time it takes for a LUN migration to complete will depend on the exact VNX
					model, workload, and volume size, and the amount of data actually stored in the
					volume. The default timeout for LUN migrations is 12 hours. If your deployment
					uses volumes >50GB, or if you find that snapshots fail and a "migration timeout"
					message is seen in the SC logs, then you should increase the timeout to a larger
					value. It is recommended that if you plan on using volumes in the 100GB range
					that you set that timeout to 3600 or larger. You can set the timeout using
						<codeph>euca-modify-property</codeph> as follows:
					<codeblock>euca-modify-property -p [partition].storage.lunmigrationtimeout=[time in hours]</codeblock></note></result>
		</taskbody>
	</task>
	<task id="enabling_san_ceph">
		<title>Enabling Ceph-RBD</title>
		<taskbody>
			<context>This task assumes the following: <ul>
					<li>You have an installed and working Ceph cluster.</li>
					<li>Ceph user credentials assigned to Eucalyptus SC and NCs are configured with
						appropriate permissions (different user credentials can be used for the SCs
						and NCs, below sections will explain how to configure them in
						Eucalyptus).</li>
				</ul>
			</context>


			<steps>
				<step>

					<cmd>Node Controllers (NCs) are designed to communicate with Ceph cluster via
						libvirt. This interaction requires a hypervisor that supports Ceph-RBD. The
						following instructions will walk you through steps for verifying and or
						installing the hypervisor. Repeat this process for every NC in the
						Eucalyptus cluster.</cmd>

					<substeps>
						<substep>
							<cmd>Verify if <codeph>qemu-kvm</codeph> and <codeph>qemu-img</codeph>
								are already installed.</cmd>
							<info><codeblock>yum list qemu-kvm qemu-img</codeblock>
								<p>Proceed to the preparing the RHEV qemu packages step if they are
									not installed.</p></info>
						</substep>

						<substep>
							<cmd>Verify qemu support for the <codeph>ceph-rbd</codeph> driver.</cmd>
							<info>
								<codeblock>qemu-img --help
qemu-img version 0.12.1, Copyright (c) 2004-2008 Fabrice Bellard
...
Supported formats: raw cow qcow vdi vmdk cloop dmg bochs vpc vvfat qcow2 qed vhdx parallels nbd blkdebug host_cdrom 
host_floppy host_device file gluster gluster gluster gluster rbd</codeblock>
								<note>If 'rbd' is listed as one of the supported formats, no further
									action is required; otherwise proceed to the next step.</note>
							</info>
						</substep>
						<substep>
							<cmd>If the <codeph>eucalyptus-nc</codeph> service is running,
								terminate/stop all instances. After all instances are terminated,
								stop the eucalyptus-nc service.</cmd>
							<info>
								<codeblock>service eucalyptus-nc stop</codeblock>
							</info>
						</substep>
						<substep>
							<cmd>Uninstall the <codeph>qemu-kvm</codeph> and
									<codeph>qemu-img</codeph> packages. </cmd>
							<info>
								<note>This will also uninstall <codeph>libvirt</codeph> if it was
									installed. Make a copy of the current
										<codeph>/etc/libvirt/libvirtd.conf</codeph> file. The yum
									uninstallation process will save the current copy to
										<codeph>/etc/libvirt/libvirtd.conf.rpmsave</codeph>.</note>
								<codeblock>yum remove qemu-kvm qemu-img</codeblock>
							</info>
						</substep>
						<substep>
							<cmd>Prepare the RHEV qemu packages:</cmd>
							<info>
								<ul>
									<li>If this NC is a RHEL system and the RHEV subscription to
										qemu packages is available, consult the RHEV package
										procedure to install the qemu-kvm-rhev and qemu-img-rhev
										packages. Blacklist the RHEV packages in the Eucalyptus repo
										to ensure that packages from the RHEV repo are installed.
										Skip to step <b>g</b> after installing the RHEV packages. </li>

									<li> If this NC is a RHEL system and RHEV subscription to qemu
										packages is unavailable, eucalyptus built and maintained
										qemu-rhev packages may be used. These packages are available
										in the same yum repository as other eucalyptus packages.
										Note that using eucalyptus built RHEV packages voids the
										original RHEL support for the qemu packages.</li>
									<li> If this NC is a non-RHEL (CentoOS) system, Eucalyptus-built
										and maintained qemu-rhev packages may be used. These
										packages are available in the same yum repository as other
										eucalyptus packages. </li>
								</ul>
							</info>
						</substep>
						<substep>
							<cmd>Install Eucalyptus-built RHEV packages:
									<codeph>qemu-kvm-rhev</codeph> and
									<codeph>qemu-img-rhev</codeph>. </cmd>
							<info>
								<codeblock>yum install qemu-kvm-rhev qemu-img-rhev</codeblock>
							</info>
						</substep>
						<substep>
							<cmd>Install <codeph>libvirt</codeph>.</cmd>
							<info>
								<codeblock>yum install libvirt</codeblock>
							</info>
						</substep>
						<substep>
							<cmd>If libvirt was uninstalled in step <b>d</b>, copy the saved
									<codeph>libvirtd.conf</codeph> file to
									<codeph>/etc/libvirt/libvirtd.conf</codeph>. Otherwise modify
								the <codeph>/etc/libvirt/libvirtd.conf</codeph> file as per your
								requirements.</cmd>
							<info>
								<codeblock>yum install libvirt</codeblock>
							</info>
						</substep>
						<substep>
							<cmd>Start the <codeph>libvirtd</codeph> service.</cmd>
							<info>
								<codeblock>service libvirtd start</codeblock>
							</info>
						</substep>
						<substep>
							<cmd>Verify <codeph>qemu</codeph> support for the
									<codeph>ceph-rbd</codeph> driver.</cmd>
							<info>
								<codeblock>qemu-img --help
qemu-img version 0.12.1, Copyright (c) 2004-2008 Fabrice Bellard
...
Supported formats: raw cow qcow vdi vmdk cloop dmg bochs vpc vvfat qcow2 qed vhdx parallels nbd blkdebug host_cdrom 
host_floppy host_device file gluster gluster gluster gluster rbd</codeblock>
							</info>
						</substep>
						<substep>
							<cmd>Make sure the eucalyptus-nc service is started.</cmd>
							<info>
								<codeblock>service eucalyptus-nc start</codeblock>
							</info>
						</substep>



					</substeps>
				</step>


				<step>
					<cmd>Configure the SC to use Ceph-RBD for block storage.</cmd>
					<info><codeblock>euca-modify-property -p &lt;partition>.storage.blockstoragemanager=ceph-rbd</codeblock>
						The output of the command should be similar to:
						<codeblock>PROPERTY	PARTI00.storage.blockstoragemanager	ceph-rbd was &lt;unset></codeblock>
					</info>
				</step>
				<step>
					<cmd>Verify that the propery value is now <codeph>ceph-rbd</codeph>:</cmd>
					<info>
						<codeblock>euca-describe-properties | grep blockstorage</codeblock>
					</info>
				</step>
				<step>
					<cmd>Check the SC to be sure that it has transitioned out of the
							<codeph>BROKEN</codeph> state and is in the <codeph>NOTREADY</codeph>,
							<codeph>DISABLED</codeph> or <codeph>ENABLED</codeph> state before
						configuring the rest of the properties for the SC.</cmd>
				</step>
				<step>
					<cmd>The ceph-rbd provider will assume defaults for the following
						properties</cmd>
					<info>
						<codeblock>euca-describe-properties -v | grep 'PARTI00.storage.ceph'
 
PROPERTY        PARTI00.storage.cephconfigfile  /etc/ceph/ceph.conf
DESCRIPTION     PARTI00.storage.cephconfigfile  Absolute path to Ceph configuration (ceph.conf) file. Default value is '/etc/ceph/ceph.conf'
 
PROPERTY        PARTI00.storage.cephkeyringfile /etc/ceph/ceph.client.eucalyptus.keyring
DESCRIPTION     PARTI00.storage.cephkeyringfile Absolute path to Ceph keyring (ceph.client.eucalyptus.keyring) file. Default value is '/etc/ceph/ceph.client.eucalyptus.keyring'
 
PROPERTY        PARTI00.storage.cephsnapshotpools       rbd
DESCRIPTION     PARTI00.storage.cephsnapshotpools       Ceph storage pool(s) made available to Eucalyptus for EBS snapshots. Use a comma separated list for configuring multiple pools. Default value is 'rbd'
 
PROPERTY        PARTI00.storage.cephuser        eucalyptus
DESCRIPTION     PARTI00.storage.cephuser        Ceph username employed by Eucalyptus operations. Default value is 'eucalyptus'
 
PROPERTY        PARTI00.storage.cephvolumepools rbd
DESCRIPTION     PARTI00.storage.cephvolumepools Ceph storage pool(s) made available to Eucalyptus for EBS volumes. Use a comma separated list for configuring multiple pools. Default value is 'rbd'</codeblock>
					</info>
				</step>
				<step>
					<cmd>The following steps are optional if the default values work for your
						cloud:</cmd>
					<substeps>
						<substep>
							<cmd>To set the Ceph username (the default value for Eucalyptus is
								'eucalyptus'): </cmd>
							<info>
								<codeblock>euca-modify-property -p &lt;cluster-name&gt;.storage.cephuser=qauser</codeblock>
							</info>
						</substep>
						<substep>
							<cmd>To set the absolute path to keyring file containing the key for the
								'eucalyptus' user (the default value is
								'/etc/ceph/ceph.client.eucalyptus.keyring'):</cmd>
							<info>
								<codeblock>euca-modify-property -p &lt;cluster-name>.storage.cephkeyringfile='/etc/ceph/ceph.client.qauser.keyring'</codeblock>
								<note>If cephuser was modified, ensure that cephkeyringfile is also
									updated with the location to the keyring for the specific
									cephuser:</note>
							</info>

						</substep>
						<substep>
							<cmd>To set the absolute path to ceph.conf file (default value is
								'/etc/ceph/ceph.conf'):</cmd>
							<info>
								<codeblock>euca-modify-property -p &lt;cluster-name>.storage.cephconfigfile=&lt;path-to-ceph.conf></codeblock>
							</info>
						</substep>
						<substep>
							<cmd>To change the comma-delimited list of Ceph pools assigned to
								Eucalyptus for managing EBS volumes (default value is 'rbd') :</cmd>
							<info>
								<codeblock>euca-modify-property -p &lt;cluster-name>.storage.cephvolumepools=rbd,qavolumes</codeblock>
							</info>
						</substep>
						<substep>
							<cmd>To change the comma-delimited list of Ceph pools assigned to
								Eucalyptus for managing EBS snapshots (default value is 'rbd')
								:</cmd>
							<info>
								<codeblock>euca-modify-property -p &lt;cluster-name>.storage.cephsnapshotpools=qasnapshots</codeblock>
							</info>
						</substep>
					</substeps>
				</step>
				<step>
					<cmd>Every NC will assume the following defaults:</cmd>
					<info>
						<codeblock>CEPH_USER_NAME="eucalyptus"
CEPH_KEYRING_PATH="/etc/ceph/ceph.client.eucalyptus.keyring"
CEPH_CONFIG_PATH="/etc/ceph/ceph.conf"</codeblock>
					</info>

					<substeps>
						<substep>
							<cmd>To override the above defaults, add/edit the following properties
								in the <codeph>$EUCALYPTUS/etc/eucalyptus/eucalyptus.conf</codeph>
								on the specific NC. file: </cmd>
							<info>
								<codeblock>CEPH_USER_NAME="&lt;ceph-username-for-use-by-this-NC>"
CEPH_KEYRING_PATH="&lt;path-to-keyring-file-for-ceph-username>"
CEPH_CONFIG_PATH="&lt;path-to-ceph.conf-file>"</codeblock>
							</info>

						</substep>

						<substep>
							<cmd>Repeat the previous two steps for every NC in the specific
								Eucalyptus cluster.</cmd>
						</substep>
					</substeps>
				</step>
			</steps>
		</taskbody>
	</task>
</task>
