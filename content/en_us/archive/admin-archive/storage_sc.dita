<?xml version="1.0" encoding="UTF-8"?>
<!--This work by Eucalyptus Systems is licensed under a Creative Commons Attribution-ShareAlike 3.0 Unported License. See the accompanying LICENSE file for more information.-->
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "concept.dtd">
<concept id="storage_sc">
	<title>Storage Controller</title>
	<shortdesc/>
	<prolog>
		<metadata>
			<keywords>
				<indexterm>storage <indexterm>Storage Controller</indexterm>
				</indexterm>
			</keywords>
		</metadata>
	</prolog>
	<conbody>

		<p>The Storage Controller (SC) is the component of Eucalyptus that
			manages EBS volumes. Eucalyptus uses the iSCSI protocol to connect
			EBS volumes to Node Controllers (NCs) and instances, and uses
			standard linux commands for configuring and exporting the volumes to
			instances. Volumes are represented as files on the filesystem of the
			machine hosting the SC. The SC manages creating, deleting,
			snapshotting, and exporting volumes in response to both user
			commands (for example: <codeph>euca-create-volume</codeph> and
				<codeph>euca-create-snapshot</codeph>) and system operations
			(examples include running a Boot from EBS (BfEBS) instance).</p>

		<p>There are two broad SC configurations: filesystem-backed SCs and
			SAN-backed SCs.</p>

		<p>For filesystem-backed SCs, the SC machine is both on the control and
			data path for the volumes and snapshots. This means the machine
			running the SC component will host the volume and snapshot data,
			manage the iSCSI connections to the volumes, and service iSCSI block
			requests as NCs (and vm instances) read and write to the volume. In
			this configuration it is important to understand and plan-for the
			I/O requirements your specific deployment will have and ensure that
			the machine hosting the SC is up to the task. We recommend lots of
			memory and as many high-speed disks as possible in a reliable and
			performant configuration (RAID 10,01,5,6 but not just 0 or 1). SSDs
			would also make an excellent addition to an SC machine due to their
			very high IOPS capability. For more recommendations on hardware
			configurations and best-practices, see our Storage:Best Practices
			section.</p>

		<section>
			<title>Backing Store (where the volumes and data are)</title>
			<p>Default Backing Store: Posix Filesystem The default configuration
				for the SC uses a standard Posix filesystem as the backing
				storage for EBS volumes. Each EBS volume is a single file and
				each snapshot is also a single file. The files for snapshots and
				volumes are not connected in any way in the filesystem (i.e.
				they are not symlinks to each other etc).</p>
			<p>Which filesystem should you use? The only requirement is that the
				filesystem is Posix compliant. We test the SC with ext4 most
				extensively, but ext3 and xfs work as well (others like btrfs
				and reiserfs will probably work but have not been tested). We
				recommend configuring the filesystem with full journaling of
				both data and metadata rather than just metadata journaling.
				Full journaling will increase the robustness of the filesystem
				and minimize chances of data-loss due to machine failure.
				Eucalyptus does not currently support a shared filesystem for
				hosting the volumes because the SC has the expectation that the
				filesystem subtree it uses is under the sole control of the SC
				local and updates to that subtree made by another SC could cause
				file conflicts that the SC does not expect. Doing so can cause
				name conflicts, particularly for snapshots because snapshot
				names (i.e. snap-abc123) are globally unique in Eucalyptus.</p>
			<p>Where are the volumes stored on the filesystem? The SC operates
				on the filesystem at $EUCALYPTUS/var/lib/eucalyptus/volumes,
				where $EUCALYPTUS is the root of the Eucalyptus installation on
				the SC machine (the default is '/' if installed by packages).
				This point in the filesystem can be provided by a separate LVM
				volume and for best performance that LVM volume should be on its
				own high-performance disks (the more spindles the better for
				random I/O performance).</p>
			<p>What is the directory structure the SC uses?All volumes and
				snapshots on a Storage Controller are stored in a single flat
				directory structure at
				$EUCALYPTUS/var/lib/eucalyptus/volumes.</p>
			<p>SAN Storage (paid subscriptions only) The SC can also be
				configured to interact with a SAN device to host EBS volumes. In
				this configuration the SC manages metadata for volumes and
				snapshots and issues commands to the SAN to perform required
				operations, but the volumes and snapshots themselves are hosted
				by the SAN and the SAN manages the iSCSI connections to the NCs.
				In this configuration the SC is not in the data path for EBS
				volume block traffic and therefore this configuration typically
				yields much better performance.</p>
		</section>
		<section>
			<title>Export Volumes (how the volumes are accessible by
				instances)</title>
			<p>SAN-backed SCs For SAN-backed SCs the SAN exports the volumes
				directly to NCs using iSCSI. This is very device specific and we
				will not discuss it further in this section. Eventually we will
				probably put up an entire section dedicated to SAN-backed SCs as
				well as the devices we support and how they are used, but that
				feature is not part of the open-source Eucalyptus.</p>
			<p>Filesystem-backed SCs SCs using a filesystem as the backend
				export volumes using standard linux iSCSI servers and tools.
				Specifically, the SC uses the linux ISCSI Framework (TGT) to
				export the volumes as iSCSI stores to the NCs. The SC creates an
				LVM volume from the file using a loopback device (more details
				below) and exports that device as a store using tgtadm.</p>
		</section>
		<section>
			<title>Snapshotting</title>
			<p>Filesystem-backed SCs A snapshot operation is a simple file-copy
				in this case. However, there are some complexities due to the
				headers added to the file when it becomes an lvm volume. For
				that reason, we snapshot by copying (using dd) the source file
				into the destination file after the destination has been created
				and configured as an lvm volume independently of the source. The
				SC only copies the final lvm volume devices so that all lvm
				header and metadata information stays unique to each file.</p>
		</section>
		<!--<section>
 		
 		<title>Lifecycle of a Request</title>
 		
 		<p>Create volume (for file system-backed Storage Controller):</p>
 		<ol>
 			<li>User issues euca-create-volume -z cluster01 -s 1</li>
 			<li>Message sent from eucatools to the Cloud Controller (CLC)</li>
 			<li>Message processed by EC2 API components in CLC, parses the message</li>
 			<li>CLC creates its own metadata about the volume to be created and gives it a name: vol-abc0123</li>
 			<li>CLC sets status of volume to 'creating' and sends a message to Storage Controller in the appropriate zone/cluster/partition to actually create the volume.</li>
 			<li>SC checks that permissions are okay, sets up some metadata of its own, and starts the asynchronous volume create operation. Then, immediately replies back to the CLC, which has been waiting for the reply.</li>
 			<!-\-<li>SC creates the volume itself using these steps below for Filesystem- or SAN-backed</li>-\->
 			<li>Assuming /dev/loop0 is open and there are no other volumes, so target-id 1 is unused as well.</li>
 			<li>Create a file: $EUCALYPTUS/var/lib/eucalyptus/volumes/vol-abc0123 of size 1GB + lvm header size (~4MB): dd -if /dev/zero -of $EUCALYPTUS/var/lib/eucalyptus/volumes/vol-X count=1 bs=1M</li>
 			<li>Find unused loopback: losetup -f</li>
 			<li>Attach file to a loopback using losetup: losetup /dev/loop0 $EUCALYPTUS/var/lib/eucalyptus/volumes</li>
 			<li>Create physical volume from loopback: pvcreate /dev/loop0.</li>
 			<li>Create volume group: vgcreate /dev/loop0 vg-xyza (the volume group name is a 4 char random hash).</li>
 			<li>Create logical volume: lvcreate -n lv-jklm -l 100%FREE vg-xyza</li>
 			<li>Create new iSCSI target: tgtadm -\-lld iscsi -\-op new -\-mode target -\-tid 1 -T
 				&lt;some_prefix>&lt;SC_NAME>:store0</li>
 			<li>Create new lun in the target: tgtadm -\-lld iscsi -\-op new -\-mode logicalunit -\-tid 1 -\-lun 1 -b /dev/vg-xyza/lv-jklm</li>
 			<li>Add eucalyptus CHAP user to target: tgtadm -\-lld iscsi -\-op bind -\-mode account -\-tid 1 -\-user eucalyptus</li>
 			<li>Bind the target to all interfaces: tgtadm -\-lld iscsi -\-op bind -\-mode target -\-tid 1 -I ALL
 			</li>
 			
 		</ol>
 		<p>Concurrently, the CLC is periodically checking the state of all volumes on every SC, so now that the volume is ready, the CLC sees it and sets the volume state to 'available' as viewable by the user via euca-describe-volumes commands.</p>
 
 		<p>Create volume (SAN-backed Storage Controller):</p>
 			
 		<ol>
 			<li>User issues euca-create-volume -z cluster01 -s 1</li>
 			<li>Message sent from eucatools to the Cloud Controller (CLC)</li>
 			<li>Message processed by EC2 API components in CLC, parses the message</li>
 			<li>CLC creates its own metadata about the volume to be created and gives it a name: vol-abc0123</li>
 			<li>CLC sets status of volume to 'creating' and sends a message to Storage Controller in the appropriate zone/cluster/partition to actually create the volume.</li>
 			<li>SC checks that permissions are okay, sets up some metadata of its own, and starts the asynchronous volume create operation. Then, immediately replies back to the CLC, which has been waiting for the reply.</li>
 			</ol>
 			<p>In this case the SC issues the proper commands to the SAN device directly to have the SAN create a LUN of the proper size and setup the proper credentials for connection later.</p>
 		<p>Concurrently, the CLC is periodically checking the state of all volumes on every SC, so now that the volume is ready, the CLC sees it and sets the volume state to 'available' as viewable by the user via euca-describe-volumes commands.</p>
 			
 	</section>-->
		<section>
			<title>Attach volume</title>
			<ol>
				<li>User runs euca-attach-volume -d /dev/sdf -i i-123abc
					vol-abc0123</li>
				<li>CLC gets message to attach the volume.</li>
				<li>CLC does some checks to make sure this is a valid operation
					(i.e. volume is not already attached)</li>
				<li>CLC finds the appropriate zone/cluster/partition where the
					volume is located.</li>
				<li>CLC sends message to SC telling it to prep the volume for
					attach and to get the connection information to send to the
					Node Controller.</li>
				<li>SC does attach stuff (see below) and returns the connection
					information necessary for clients to attach. For
					filesystem-backed SC, this is essentially a No-Op. For
					SAN-backed SC, the SC issues the proper commands to the SAN
					to configure credentials and expose the proper LUN to the
					proper NC.</li>
				<li>CLC gets response and sends request to Cluster Controller
					(CC) in proper zone/cluster/partition to do the attach of
					the volume to the instance.</li>
				<li>CC looks up the Node Controller (NC) hosting the instance
					requested</li>
				<li>CC sends the doAttach message to the proper NC including the
					connection info.</li>
				<li>NC uses the connection info to connect its local iSCSI
					initiator to the iscsi target on the SC</li>
				<li>NC has connection to the iSCSI target and has a local
					hypervisor block-device (i.e. /dev/sdd)</li>
				<li>NC exposes that block device to the instance via libvirt and
					does proper mapping of device to user-desired name (i.e.
					NC's /dev/sdd becomes the instance's /dev/sdf, as per the
					original user request).</li>

			</ol>
		</section>
	</conbody>
</concept>
